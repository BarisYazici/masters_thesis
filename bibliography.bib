@book{latex,
  title = {LaTeX : A Documentation Preparation System User's Guide and Reference Manual},
  publisher = {Addison-Wesley Professional},
  year = {1994},
  author = {Leslie Lamport}
}

@article{Kroemer2019,
  abstract = {A key challenge in intelligent robotics is creating robots that are capable of directly interacting with the world around them to achieve their goals. The last decade has seen substantial growth in research on the problem of robot manipulation, which aims to exploit the increasing availability of affordable robot arms and grippers to create robots capable of directly interacting with the world to achieve their goals. Learning will be central to such autonomous systems, as the real world contains too much variation for a robot to expect to have an accurate model of its environment, the objects in it, or the skills required to manipulate them, in advance. We aim to survey a representative subset of that research which uses machine learning for manipulation. We describe a formalization of the robot manipulation learning problem that synthesizes existing research into a single coherent framework and highlight the many remaining research opportunities and challenges.},
  archivePrefix = {arXiv},
  arxivId = {1907.03146},
  author = {Kroemer, Oliver and Niekum, Scott and Konidaris, George},
  eprint = {1907.03146},
  file = {:Users/barisyazici/Desktop/Grasping/A Review of Robot Learning for Manipulation- Challenges, Representations, and Algorithms.pdf:pdf},
  mendeley-groups = {Grasping},
  title = {{A Review of Robot Learning for Manipulation: Challenges, Representations, and Algorithms}},
  url = {http://arxiv.org/abs/1907.03146},
  year = {2019}
}

@book{Sutton2018,
  abstract = {Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability.The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  booktitle = {The MIT Press},
  file = {:Users/barisyazici/Desktop/books/ReinforcementSutton.pdf:pdf},
  isbn = {9780262039246},
  mendeley-groups = {RL general},
  pages = {1--3},
  title = {{Reinforcement Learning, Second Edition: An Introduction - Complete Draft}},
  year = {2018}
}

@article{AnimalInt11,
  title = {Animal Intelligence: Experimental Studies.},
  publisher = {New York: The Macmillan Co.; London: Macmillan and Co., Ltd., 1911.},
  year = {1911.},
  author = { E. L. Thorndike.}
}

@book{Gagniuc2017,
  abstract = {around examples based on objects such as jars (representing states) and balls (representing transition probabilities) of various colors. Thus, any type of Markov chain configuration is explained in terms of real experiments. These examples relate to each other from chapter to chapter enabling a gradual under- standing of the phenomena. The theory is also accompanied by an algorithm implementation for each example. Chapter 1 begins with a general introduction into the history of probability theory, covering different time periods. In this chapter, the introduction to discrete-time is made using quantifiable examples showing howthe field ofprobability theory arrived in recent times at the notion of dependent variables (Markov model) from experiments related to indepen- dent variables (Bernoulli model). Chapters 2 and 3 are an introduction to simple stochastic matrices and transition probabilities followed by a simulation of a two-state Markov chain. The description starts from the observation of events within a system up to the simulation of that system through a Markov chain. The construction of a stochastic matrix is shown based on both a sequence of observations and observations provided in percentages. Chapter 4 begins with an introduction to predictions that use a two-state Markov chain. Here, the notion of steady state is first approached and discussed in connection with the long-run distribution behavior of the Markov chain. Chapter 5 describes some examples by considering predictions based on Markov chains with more than two states. The first two examples include a three-state Markov chain and a four-state Markov chain, after which a gradual generalization is made for an arbitrary number of states (n-states). In Chapter 6, the notion of absorbing Markov chains it is approached by using tangible examples. Chapter 7 covers a topic linked to the average time spent in a state, whereas Chapter 8 covers dis- cussions on different configurations of chains. Different configurations of state diagrams provide solutions for different problems in many fields. As a continu- ation, Chapter 9 covers the simulation ofan n-state Markov chain used for veri- fying experiments ofvarious diagram configurations. Overall, the book intends a completely different approach on Markov chains, which is based on four convergent lines that include mathematics, implementation, simulation, and experimentation.},
  author = {Gagniuc, Paul A},
  file = {:Users/barisyazici/Downloads/Markov chains from theory to implementation and experimentation by Gagniuc, Paul A (z-lib.org).pdf:pdf},
  isbn = {9781119387572},
  mendeley-groups = {RL general},
  publisher = {Wiley},
  title = {{From Theory to Implementation and Experimentation}},
  year = {2017}
}

@unpublished{PerezMIT,
  author = {Tomás Lozano-Pérez, and Leslie Kaelbling},
  title  = {6.825 Techniques in Artificial Intelligence (SMA 5504)},
  note   = {Massachusetts Institute of Technology: MIT OpenCourseWare, https://ocw.mit.edu},
  year   = {Fall 2002},
  url    = {https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-825-techniques-in-artificial-intelligence-sma-5504-fall-2002/index.htm#}
}

@article{Bellman1958,
  abstract = {Consider a system S specified at any time t by a finite dimensional vector x(t) satisfying a vector differential equation dx/dt = g[x, r(t), f(t)], x(0) = c, where c is the initial state, r(t) is a random forcing term possessing a known distribution, and f(t) is a forcing term chosen, via a feedback process, so as to minimize the expected value of a functional J(x) = f{\{}hook{\}}0T h(x - y, t) dG(t), where y(t) is a known function, or chosen so as to minimize the functional defined by the probability that max0 ≦ t ≦ T h(x - y, t) exceed a specified bound. It is shown how the functional equation technique of dynamic programming may be used to obtain a new computational and analytic approach to problems of this genre. The limited memory capacity of present-day digital computers limits the routine application of these techniques to first and second order systems at the moment, with limited application to higher order systems. {\textcopyright} 1958.},
  author = {Bellman, Richard},
  doi = {10.1016/S0019-9958(58)80003-0},
  file = {:Users/barisyazici/Downloads/1-s2.0-S0019995858800030-main.pdf:pdf},
  issn = {00199958},
  journal = {Information and Control},
  mendeley-groups = {RL general},
  number = {3},
  pages = {228--239},
  title = {{Dynamic programming and stochastic control processes}},
  volume = {1},
  year = {1958}
}

@book{Borovcnik1991,
  abstract = {Generative probability models such as hidden Markov models provide$\backslash$na principled way of treating missing information and dealing with$\backslash$nvariable length sequences. On the other hand, discriminative $\backslash$n$\backslash$nmethods such as support vector machines enable us to construct flexible$\backslash$ndecision boundaries and often result in classification performance$\backslash$nsuperior to that of the model based approaches. An ideal classifier$\backslash$nshould combine these two complementary approaches. In this paper,$\backslash$nwe develop a natural way of achieving this combination by deriving$\backslash$nkernel functions for use in discriminative methods such as support$\backslash$nvector machines from generative probability models. We provide a$\backslash$ntheoretical justication for this combination as well as demonstrate$\backslash$na substantial improvement in the classifcation performance in the$\backslash$ncontext of DNA and protein sequence analysis.},
  author = {Borovcnik, Manfred and Bentz, Hans-Joachim and Kapadia, Ramesh},
  booktitle = {Chance Encounters: Probability in Education},
  doi = {10.1007/978-94-011-3532-0_2},
  file = {:Users/barisyazici/Desktop/docs/TUM/donemler/2017W/MachineLearning/Books/Murphy.pdf:pdf},
  isbn = {9780262018029},
  mendeley-groups = {Machine Learning},
  pages = {27--71},
  title = {{A Probabilistic Perspective}},
  year = {1991}
}

@book{Bishop06,
  author = {Bishop, Christopher M.},
  title = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
  year = {2006},
  isbn = {0387310738},
  publisher = {Springer-Verlag},
  address = {Berlin, Heidelberg}
}
  
@book{Murphy12,
  author = {Murphy, Kevin P.},
  title = {Machine Learning: A Probabilistic Perspective},
  year = {2012},
  isbn = {0262018020},
  publisher = {The MIT Press}
}
  

