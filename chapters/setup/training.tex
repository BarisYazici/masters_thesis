\section{Training Structure}

As described in the hardware setup section, we train on LRZ compute cloud mainly. Given that we cannot monitor the status of training always, we obliged to implement a robust training structure to overcome the unexpected death of the processes[\todo{better}]. Also, in general, machine learning by nature is not always improving throughout the training process. The model performance might well get worse towards the end of the training. One way to avoid this problem is to implement checkpoint and evaluation callbacks to inspect the model performance at regular intervals. Another way to achieve robust training is to monitor the training process and log the results to a CSV file. This way, we can overview the training process and judge where to end the training. We primarily monitor the training loss, success rate, and reward.

With the help of evaluation callback, we validate the model performance on a different instance of the gripper environment with the validation object set. We regularly save the best performing model based on average reward over ten episodes. This process assures that we will always have the best-generalized model. If the training unexpectedly ends or the loss increases, we would have a model to judge the performance. We repeat the training process evaluation every 50000 timesteps. Besides, we save the agent model every 25000 timesteps to assure a minimum loss in case of a sudden stop of the training process.
The monitoring procedure is taken care of by the modified version of Stable Baselines monitor wrapper.  We extended the wrapper class\footnote{\url{https://bit.ly/2CtoEX1}} to save the timestep, success rate, and curriculum lambda in addition to reward and episode information.


\begin{lstlisting}[language=Python, caption= Integration of monitor wrapper helper function, label=code:monitor]


    Monitor(gym.make('gripper-env-v0', config=config), "log_file")


\end{lstlisting}

Moreover, we implemented a parameterized training structure to allow users to try different configurations quickly. We parameterized input and reward normalization and time-feature wrapper.
