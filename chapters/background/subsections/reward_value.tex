\subsection{Reward and Value function}
\todo{Ilk paragraf} 

As defined in the Markov Chain section, rewards and value functions are the essence of value iteration algorithms. In this section, we will describe the objective of the RL problem formally. As we mentioned in previous chapters, we want the increase the number of rewards we achieve when we reach the terminal state. If we define the reward at final state \(T\) as \(R_T\) and the reward at the initial state \(t\) is \(R_t\), returns we receive is the sum of rewards in every state.

\begin{equation}
    G_t = R_{t+1} + R_{t+2} + R_{t+3} + ... + R_T    
\end{equation}


\(G_t\) is the notation of expected return. We will mostly use the discounted version of the expected return calculation. We introduce a discount factor (\(\gamma\), to value the rewards that the agent receives now, compare to the rewards in the future. 
\begin{equation}
    G_t = R_{t+1} + \gamma^1*R_{t+2} + \gamma^2*R_{t+3} + ... + \gamma^T-1*R_T = \sum\limits_{k=0}\gamma^kR_{t+k+1}
\end{equation}

For instance, if a rational human offered 1m Euros now, versus 1m Euros in 50 years, would usually choose 1m Euros now. Therefore, our agent also weighs the rewards it receives now, over 50 steps from the current state.  In the meantime, we do not want the agent to undervalue the importance of reaching the terminal state through the highest reward sequence.  Given the below example, if we introduce a discount factor of 0, the agent will always try to maximize the immediate reward and take a sequence of s1-a2-s3-a4-s4 and end up with non-optimal greedy algorithm with \(G_t = 20 \). But with a discount factor, in this example everything between \(0< \gamma < 1\) works, would find the optimal sequence(s1-a1-s2-a3-s4) with \(G_t = 25\).

The value function is the expectation of returns while an agent is following the policy (\(\pi\)). Policy namely represents the probability distribution of an agent taking action \(a\) in state \(s\). The policy is similar to the transition probability matrix in the Markov Chain section. Using policy, we can define the value function of an agent following the policy pi as below.