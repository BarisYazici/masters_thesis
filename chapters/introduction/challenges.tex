\section{Challenges}
    \begin{enumerate}
        \item \textbf{Reproducibility} -   Reproducibility is a known issue on machine learning-based approaches. They are never guaranteed to find the global optimum. They are even never guaranteed to find the same local optimum over different trials. Our grasping simulator is random by nature. In our stochastic environment, reproducibility issues can be overwhelming. We cannot verify the hyperparameters' relative value if the performance changes tremendously over different trials. The situation is similar in the low-level. Stable baselines documentation\footnote{\url{https://bit.ly/3jwxhQH}} 
        mention that they cannot even assure the same performance in CPU if the model is trained in GPU. Therefore, we needed to spend more time getting the mean value by running the same model for multiple instances.
        \item \textbf{Hyperparameter Optimization} - Similar to the reproducibility issue, hyperparameter optimization is a very well-known issue in machine learning. No theoretical proofs are saying how hyperparameters need to be tuned. The  only solution is to assign the parameters based on some heuristics intuitively. In our work, we used a hyperparameter optimization library to handle this problem. Some trials took up to 80 trials over a week to deliver the most optimized parameters.
        \item \textbf{GPU memory optimization} - GPU memory is the most valuable asset for deep learning applications. If one utilizes the GPU efficiently, one can run more experiments in parallel. Thus, we separated most of our time to fix the encoder or algorithm memory leaks. These leaks clog the GPU memory and lead to unexpected training run shutdowns. That is why we put a lot of effort to minimize the GPU memory usage and maximize memory utilization.
        \item \textbf{Keras and Tensorflow mismatch} - Keras is a high-level, user-friendly interface built on Tensorflow backend. Using both Keras and Tensorflow leads to complicated scenarios, where computation graphs variables can be mixed up. We encountered that the original BDQ algorithm attempted to save the Keras variables used for the autoencoder implementation with the actual Tensorflow computation graph variables. The overall situation caused a malfunction in the trained model's save and load functionality. We solved the issue by adequately namespacing every graph variable and tediously saving them in the BDQ algorithm's save method.
        % \item \textbf{Correct graphics card driver version}
    \end{enumerate}
  
    